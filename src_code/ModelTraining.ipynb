{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71257b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from custom_model.predictor import *\n",
    "from custom_model.losses import *\n",
    "from custom_model.datagen import *\n",
    "from custom_model.inference import *\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "from config import *\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this section to train an ensemble of maximum 15 beta-regression models\n",
    "'''\n",
    "Parameters are stored in para(Dict):\n",
    "    para['mode'] = {'dgc', 'spatialattention','normalgc'}\n",
    "    para['modelnorm'] = {'batch', 'layer'}\n",
    "    para['output_form'] = {'beta', 'histogram'}\n",
    "'''\n",
    "para['mode'] = 'spatialattention' # spatial attention module\n",
    "para['modelnorm'] = 'batch' # batch normalization OR layer normalization\n",
    "para['epoches'] = 64 # feature dimension F\n",
    "para['output_form'] = 'beta' # beta output distribution\n",
    "para['normalize'] = 1 # normalize the input by z-score OR not\n",
    "para['batch_size'] = 8\n",
    "para['adajacency_range'] = 5\n",
    "para['nb_classes'] = 131\n",
    "para['nb_blocks'] = 10\n",
    "para['pred']=10\n",
    "para['interval'] = 1 # sliding window step\n",
    "para['time_length'] = 5 # tempral convolution kernel length\n",
    "\n",
    "BATCH_SIZE = para['batch_size']\n",
    "EPOCH_NUMBER = para['epochs']\n",
    "train_gen = DataGenerator(para, BATCH_SIZE, 'train')\n",
    "val_gen = DataGenerator(para, BATCH_SIZE, 'val')\n",
    "\n",
    "def scheduler(epoch):\n",
    "   return 0.001*tf.math.exp(-epoch*0.07)\n",
    "    \n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "for i in range(15):\n",
    "    model = build_crossmodel(para)\n",
    "    model.compile(loss = nll_beta(),\n",
    "                  optimizer=Adam())\n",
    "\n",
    "    history = model.fit(train_gen, validation_data = val_gen, validation_freq=3,\n",
    "              epochs=30, callbacks=[callback], verbose='auto')\n",
    "    model.save_weights('./DE/beta/model'+str(i)+'/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce216e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this section to train several histogram-regression models\n",
    "'''\n",
    "Not necessary to train too many, it is very slow and requires higher memory\n",
    "'''\n",
    "para['mode'] = 'dgc'\n",
    "para['modelnorm'] = 'batch'\n",
    "para['epoches'] = 64\n",
    "para['output_form'] = 'histogram'\n",
    "para['normalize'] = 1\n",
    "para['batch_size'] = 8\n",
    "para['adajacency_range'] = 5\n",
    "para['nb_classes'] = 131\n",
    "para['nb_blocks'] = 10\n",
    "para['pred']=10\n",
    "para['interval'] = 1\n",
    "para['time_length'] = 5\n",
    "\n",
    "BATCH_SIZE = para['batch_size']\n",
    "EPOCH_NUMBER = para['epochs']\n",
    "train_gen = DataGenerator(para, BATCH_SIZE, 'train')\n",
    "val_gen = DataGenerator(para, BATCH_SIZE, 'val')\n",
    "\n",
    "def scheduler(epoch):\n",
    "   return 0.001*tf.math.exp(-epoch*0.085)\n",
    "    \n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "for i in range(5):\n",
    "    model = build_model(para)\n",
    "    model.compile(loss = focal_loss(),\n",
    "                  optimizer=Adam())\n",
    "\n",
    "    history = model.fit(train_gen, validation_data = val_gen, validation_freq=3,\n",
    "              epochs=30, callbacks=[callback], verbose='auto')\n",
    "    model.save_weights('./DE/histogramf/model'+str(i)+'/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf54aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example code to run the inference and save the parameters for further analysis\n",
    "# if you use the provided pre-trained models, directly run this\n",
    "\n",
    "test_gen = DataGenerator(para, BATCH_SIZE, 'testin')\n",
    "A, B, Y = EnsembleInference(test_gen, para, nb_ensemble=10)\n",
    "EntropyUncertainty(a, b)\n",
    "np.savez_compressed('./result/2019', A=A, B=B, Y=Y)\n",
    "\n",
    "test_gen = DataGenerator(para, BATCH_SIZE, 'testout')\n",
    "A, B, Y = EnsembleInference(test_gen, para, nb_ensemble=10)\n",
    "np.savez_compressed('./result/2022', A=A, B=B, Y=Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
